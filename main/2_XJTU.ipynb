{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-11T02:39:17.145703Z",
     "start_time": "2025-03-11T02:39:13.814232Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset,TensorDataset,DataLoader,random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import torch.nn.init as init\n",
    "d2l.use_svg_display()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:39:29.388330Z",
     "start_time": "2025-03-11T02:39:29.256820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Batch_list=['Batch-1','Batch-2','Batch-3','Batch-4','Batch-5','Batch-6']\n",
    "def choose_batch(Batch_name):\n",
    "    Battery_list=[]\n",
    "    dir='../source_data/XJTU_data/'+Batch_name\n",
    "    for name in os.listdir(dir):\n",
    "        Battery_list.append(name.strip('.mat'))\n",
    "    source_data_path='../source_data/XJTU_data/'+Batch_name+'/'\n",
    "    target_data_path='../data/XJTU_data/'+Batch_name+'/'+'all/'\n",
    "    return Battery_list,source_data_path,target_data_path\n",
    "Battery_list,source_data_path,target_data_path=choose_batch(Batch_list[0])\n",
    "print(Battery_list)\n",
    "def smooth_data(sequence, window_size):\n",
    "    \"\"\"数据平滑\"\"\"\n",
    "    if window_size < 1:\n",
    "        raise ValueError(\"窗口大小必须大于等于1\")\n",
    "    # 初始化平滑后的数据列表\n",
    "    smoothed_sequence = []\n",
    "    # 计算窗口内的平均值\n",
    "    for i in range(len(sequence)):\n",
    "        # 计算窗口的起始和结束索引\n",
    "        start_index = max(0, i - window_size + 1)\n",
    "        end_index = i + 1\n",
    "        # 计算窗口内的数据平均值\n",
    "        window_average = sum(sequence[start_index:end_index]) / (end_index - start_index)\n",
    "        # 将平均值添加到平滑后的数据列表中\n",
    "        smoothed_sequence.append(window_average)\n",
    "    return smoothed_sequence\n",
    "def add_row_index_to_array(arr):\n",
    "    \"\"\"\n",
    "    在输入数组的每一行的第一个元素加上行号，并扩展数组维度。\n",
    "    \n",
    "    参数:\n",
    "    arr (np.ndarray): 形状为 (n, 6) 的输入数组。\n",
    "    \n",
    "    返回:\n",
    "    np.ndarray: 形状为 (n, 7) 的数组。\n",
    "    \"\"\"\n",
    "    # 检查输入数组形状是否为 (n, 6)\n",
    "    if arr.shape[1] != 10:\n",
    "        raise ValueError(\"输入数组必须是形状为 (n, 10) 的数组。\")\n",
    "\n",
    "    # 创建一个新数组，其形状为 (n, 7)，初始化为输入数组\n",
    "    new_arr = np.zeros((arr.shape[0], 11))\n",
    "    new_arr[:, 1:] = arr  # 将输入数组的数据复制到新数组的后面六个列\n",
    "    # 在新数组的每一行的第一个元素加上行号\n",
    "    new_arr[:, 0] = np.arange(arr.shape[0])\n",
    "    return new_arr\n",
    "def make_sequences(text, window_size):\n",
    "    \"\"\"用容量作为文本序列text,window是窗口的大小\"\"\"\n",
    "    x, y = [],[]\n",
    "    for i in range(len(text) - window_size):\n",
    "        sequence = text[i:i+window_size]\n",
    "        target = text[i+window_size]\n",
    "\n",
    "        x.append(sequence)\n",
    "        y.append(target)\n",
    "    return np.array(x), np.array(y)\n",
    "def drop_outlier(array,count,bins):\n",
    "    \"\"\"离群值提取--用3sigma方法\"\"\"\n",
    "    index = []\n",
    "    range_n = np.arange(1,count,bins)\n",
    "    for i in range_n[:-1]:\n",
    "        array_lim = array[i:i+bins]\n",
    "        sigma = np.std(array_lim)\n",
    "        mean = np.mean(array_lim)\n",
    "        th_max,th_min = mean + sigma*2, mean - sigma*2\n",
    "        idx = np.where((array_lim < th_max) & (array_lim > th_min))\n",
    "        idx = idx[0] + i\n",
    "        index.extend(list(idx))\n",
    "    return np.array(index)\n",
    "def clean_data(array_figs,array_labels):\n",
    "    index_keep=drop_outlier(array_labels,len(array_labels),35)\n",
    "    array_figs,array_labels=array_figs[index_keep],array_labels[index_keep]\n",
    "    array_figs,array_labels=array_figs[drop_outlier(array_labels,len(array_labels),10)],array_labels[drop_outlier(array_labels,len(array_labels),10)]\n",
    "    return array_figs,array_labels\n",
    "data_root=target_data_path\n",
    "for name in Battery_list:\n",
    "    path=data_root+name+'.npz'\n",
    "    arrays=np.load(path)\n",
    "    features,SOHs=clean_data(arrays['array1'],arrays['array2'])\n",
    "    plt.plot(SOHs)\n",
    "    # \"\"\"抛弃异常值处理\"\"\"\n",
    "    # index_keep=drop_outlier(SOHs,len(SOHs),35)\n",
    "    # plt.plot(SOHs[index_keep][drop_outlier(SOHs[index_keep],len(SOHs[index_keep]),10)])\n",
    "    # plt.plot(process_sequence(SOHs,1))\n",
    "    plt.xlabel('Cycle')\n",
    "    plt.ylabel('Capacity')\n",
    "    # print(features[1][0])\n",
    "# plt.show()"
   ],
   "id": "aaf57521f5e3078c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2C_battery-1', '2C_battery-2', '2C_battery-3', '2C_battery-4', '2C_battery-5', '2C_battery-6', '2C_battery-7', '2C_battery-8']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"415.531025pt\" height=\"310.86825pt\" viewBox=\"0 0 415.531025 310.86825\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2025-03-11T10:39:29.366796</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 310.86825 \nL 415.531025 310.86825 \nL 415.531025 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 273.312 \nL 407.26375 273.312 \nL 407.26375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m90e6432127\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"66.376477\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(63.195227 287.910438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"121.778277\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(115.415777 287.910438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"177.180076\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(167.636326 287.910438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"232.581876\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(223.038126 287.910438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"287.983676\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(278.439926 287.910438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"343.385475\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(333.841725 287.910438) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m90e6432127\" x=\"398.787275\" y=\"273.312\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 300 -->\n      <g transform=\"translate(389.243525 287.910438) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- Cycle -->\n     <g transform=\"translate(215.038125 301.588563) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-43\" d=\"M 4122 4306 \nL 4122 3641 \nQ 3803 3938 3442 4084 \nQ 3081 4231 2675 4231 \nQ 1875 4231 1450 3742 \nQ 1025 3253 1025 2328 \nQ 1025 1406 1450 917 \nQ 1875 428 2675 428 \nQ 3081 428 3442 575 \nQ 3803 722 4122 1019 \nL 4122 359 \nQ 3791 134 3420 21 \nQ 3050 -91 2638 -91 \nQ 1578 -91 968 557 \nQ 359 1206 359 2328 \nQ 359 3453 968 4101 \nQ 1578 4750 2638 4750 \nQ 3056 4750 3426 4639 \nQ 3797 4528 4122 4306 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"69.824219\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"129.003906\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"183.984375\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"211.767578\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path id=\"mcd81018f07\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"251.045189\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 1.65 -->\n      <g transform=\"translate(20.878125 254.844408) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" transform=\"scale(0.015625)\"/>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-36\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"214.720865\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.70 -->\n      <g transform=\"translate(20.878125 218.520084) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"178.396541\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.75 -->\n      <g transform=\"translate(20.878125 182.195759) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-37\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"142.072216\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.80 -->\n      <g transform=\"translate(20.878125 145.871435) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"105.747892\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.85 -->\n      <g transform=\"translate(20.878125 109.547111) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-38\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"69.423568\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.90 -->\n      <g transform=\"translate(20.878125 73.222786) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-39\" d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use xlink:href=\"#mcd81018f07\" x=\"50.14375\" y=\"33.099243\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 1.95 -->\n      <g transform=\"translate(20.878125 36.898462) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-39\" x=\"95.410156\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- Capacity -->\n     <g transform=\"translate(14.798438 162.107563) rotate(-90) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"69.824219\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"131.103516\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"194.580078\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"255.859375\"/>\n      <use xlink:href=\"#DejaVuSans-69\" x=\"310.839844\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"338.623047\"/>\n      <use xlink:href=\"#DejaVuSans-79\" x=\"377.832031\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path d=\"M 66.376477 20.748973 \nL 67.484513 22.201946 \nL 68.592549 22.201946 \nL 69.700585 22.928432 \nL 70.808621 22.928432 \nL 71.916657 23.654919 \nL 73.024693 22.201946 \nL 74.132729 20.022486 \nL 75.240765 19.296 \nL 76.348801 20.022486 \nL 78.564873 20.022486 \nL 80.780945 21.475459 \nL 82.997017 21.475459 \nL 85.213089 22.928432 \nL 86.321125 22.201946 \nL 87.429161 22.201946 \nL 88.537197 19.296 \nL 89.645233 21.475459 \nL 94.077377 24.381405 \nL 95.185413 24.381405 \nL 96.293449 25.107892 \nL 97.401485 25.107892 \nL 98.509521 25.834378 \nL 99.617557 25.107892 \nL 100.725593 25.834378 \nL 101.833629 25.107892 \nL 102.941665 25.107892 \nL 104.049701 25.834378 \nL 105.157737 25.834378 \nL 106.265773 26.560865 \nL 107.373809 28.013838 \nL 108.481845 28.740324 \nL 109.589881 28.740324 \nL 110.697917 27.287351 \nL 111.805953 26.560865 \nL 112.913989 26.560865 \nL 114.022025 27.287351 \nL 115.130061 26.560865 \nL 119.562205 29.466811 \nL 120.670241 29.466811 \nL 122.886313 30.919784 \nL 125.102385 28.013838 \nL 127.318457 28.013838 \nL 128.426493 28.740324 \nL 129.534529 28.740324 \nL 131.750601 30.193297 \nL 132.858637 30.193297 \nL 133.966673 30.919784 \nL 135.074709 30.919784 \nL 136.182745 31.64627 \nL 139.506853 31.64627 \nL 140.614889 32.372757 \nL 141.722925 32.372757 \nL 143.938997 35.278703 \nL 146.155069 35.278703 \nL 147.263105 33.82573 \nL 148.371141 33.099243 \nL 151.695249 35.278703 \nL 152.803285 35.278703 \nL 158.343465 38.911135 \nL 159.451501 38.911135 \nL 160.559537 36.005189 \nL 161.667573 33.82573 \nL 162.775609 33.099243 \nL 171.639896 38.911135 \nL 172.747932 38.184649 \nL 173.855968 38.911135 \nL 174.964004 38.911135 \nL 176.07204 39.637622 \nL 177.180076 41.090595 \nL 178.288112 41.817081 \nL 179.396148 41.817081 \nL 184.936328 45.449514 \nL 186.044364 45.449514 \nL 187.1524 46.176 \nL 188.260436 46.176 \nL 190.476508 47.628973 \nL 191.584544 47.628973 \nL 193.800616 49.081946 \nL 194.908652 49.081946 \nL 196.016688 49.808432 \nL 197.124724 49.808432 \nL 198.23276 50.534919 \nL 199.340796 50.534919 \nL 200.448832 51.261405 \nL 201.556868 51.261405 \nL 202.664904 51.987892 \nL 203.77294 51.987892 \nL 204.880976 52.714378 \nL 205.989012 52.714378 \nL 208.205084 54.167351 \nL 209.31312 55.620324 \nL 210.421156 56.346811 \nL 211.529192 56.346811 \nL 212.637228 57.799784 \nL 219.285444 62.158703 \nL 220.39348 62.158703 \nL 223.717588 64.338162 \nL 224.825624 65.791135 \nL 229.257768 68.697081 \nL 230.365804 70.150054 \nL 231.47384 70.876541 \nL 232.581876 72.329514 \nL 233.689912 73.056 \nL 234.797948 75.961946 \nL 237.01402 77.414919 \nL 239.230092 80.320865 \nL 240.338128 81.047351 \nL 243.662236 85.40627 \nL 246.986344 87.58573 \nL 248.09438 89.038703 \nL 250.310452 90.491676 \nL 251.418488 96.303568 \nL 252.526524 98.483027 \nL 253.63456 99.209514 \nL 254.742596 97.030054 \nL 255.850632 98.483027 \nL 256.958668 98.483027 \nL 258.066704 95.577081 \nL 259.17474 96.303568 \nL 261.390812 96.303568 \nL 262.498848 97.756541 \nL 263.606884 98.483027 \nL 264.71492 99.936 \nL 265.822956 99.936 \nL 266.930992 105.747892 \nL 268.039028 107.200865 \nL 269.147064 106.474378 \nL 270.2551 107.200865 \nL 272.471172 107.200865 \nL 273.579208 109.380324 \nL 274.687244 110.106811 \nL 275.79528 111.559784 \nL 278.011352 113.012757 \nL 279.119388 112.28627 \nL 280.227424 112.28627 \nL 282.443496 113.739243 \nL 283.551532 116.645189 \nL 290.199748 125.363027 \nL 292.41582 123.910054 \nL 293.523855 124.636541 \nL 294.631891 126.089514 \nL 295.739927 128.268973 \nL 296.847963 127.542486 \nL 297.955999 127.542486 \nL 299.064035 128.995459 \nL 300.172071 133.354378 \nL 301.280107 134.807351 \nL 302.388143 136.986811 \nL 303.496179 137.713297 \nL 305.712251 136.260324 \nL 306.820287 136.986811 \nL 307.928323 139.16627 \nL 309.036359 139.892757 \nL 310.144395 142.798703 \nL 311.252431 142.798703 \nL 312.360467 143.525189 \nL 313.468503 146.431135 \nL 314.576539 148.610595 \nL 315.684575 149.337081 \nL 316.792611 150.790054 \nL 319.008683 155.148973 \nL 320.116719 156.601946 \nL 321.224755 159.507892 \nL 322.332791 160.960865 \nL 323.440827 163.140324 \nL 324.548863 164.593297 \nL 325.656899 166.772757 \nL 326.764935 169.678703 \nL 328.981007 174.037622 \nL 330.089043 174.764108 \nL 332.305115 179.123027 \nL 333.413151 180.576 \nL 335.629223 182.028973 \nL 336.737259 184.934919 \nL 337.845295 186.387892 \nL 338.953331 189.293838 \nL 340.061367 188.567351 \nL 341.169403 190.746811 \nL 342.277439 195.832216 \nL 343.385475 195.832216 \nL 344.493511 197.285189 \nL 345.601547 199.464649 \nL 346.709583 200.191135 \nL 347.817619 201.644108 \nL 348.925655 202.370595 \nL 350.033691 202.370595 \nL 351.141727 204.550054 \nL 352.249763 206.003027 \nL 353.357799 208.182486 \nL 354.465835 208.908973 \nL 355.573871 211.814919 \nL 356.681907 212.541405 \nL 357.789943 213.994378 \nL 358.897979 214.720865 \nL 358.897979 214.720865 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path d=\"M 66.376477 22.928432 \nL 67.484513 24.381405 \nL 68.592549 25.107892 \nL 70.808621 25.107892 \nL 71.916657 25.834378 \nL 73.024693 23.654919 \nL 74.132729 22.201946 \nL 75.240765 21.475459 \nL 76.348801 22.201946 \nL 77.456837 22.201946 \nL 79.672909 23.654919 \nL 80.780945 23.654919 \nL 82.997017 25.107892 \nL 84.105053 25.107892 \nL 85.213089 24.381405 \nL 86.321125 21.475459 \nL 87.429161 23.654919 \nL 88.537197 26.560865 \nL 89.645233 26.560865 \nL 90.753269 27.287351 \nL 95.185413 27.287351 \nL 96.293449 26.560865 \nL 98.509521 28.013838 \nL 99.617557 28.013838 \nL 100.725593 30.193297 \nL 102.941665 31.64627 \nL 105.157737 28.740324 \nL 106.265773 28.013838 \nL 107.373809 28.740324 \nL 109.589881 28.740324 \nL 112.913989 30.919784 \nL 114.022025 30.919784 \nL 115.130061 31.64627 \nL 116.238097 30.193297 \nL 117.346133 30.193297 \nL 120.670241 32.372757 \nL 121.778277 32.372757 \nL 122.886313 33.099243 \nL 123.994349 33.099243 \nL 125.102385 32.372757 \nL 126.210421 33.099243 \nL 127.318457 32.372757 \nL 128.426493 33.099243 \nL 129.534529 33.099243 \nL 130.642565 33.82573 \nL 131.750601 33.82573 \nL 133.966673 35.278703 \nL 135.074709 35.278703 \nL 137.290781 33.82573 \nL 138.398817 35.278703 \nL 140.614889 36.731676 \nL 141.722925 36.731676 \nL 143.938997 38.184649 \nL 145.047033 38.184649 \nL 146.155069 38.911135 \nL 147.263105 40.364108 \nL 148.371141 38.911135 \nL 149.479177 35.278703 \nL 150.587213 33.82573 \nL 151.695249 33.82573 \nL 152.803285 35.278703 \nL 153.911321 36.005189 \nL 155.019357 37.458162 \nL 156.127393 37.458162 \nL 158.343465 38.911135 \nL 159.451501 38.911135 \nL 161.667573 40.364108 \nL 162.775609 39.637622 \nL 163.883645 39.637622 \nL 164.99168 40.364108 \nL 166.099716 40.364108 \nL 167.207752 41.817081 \nL 169.423824 43.270054 \nL 170.53186 43.270054 \nL 172.747932 44.723027 \nL 173.855968 44.723027 \nL 177.180076 46.902486 \nL 179.396148 46.902486 \nL 181.61222 48.355459 \nL 182.720256 48.355459 \nL 184.936328 49.808432 \nL 186.044364 49.808432 \nL 188.260436 51.261405 \nL 189.368472 51.261405 \nL 190.476508 51.987892 \nL 191.584544 51.987892 \nL 192.69258 52.714378 \nL 193.800616 52.714378 \nL 196.016688 54.167351 \nL 197.124724 54.167351 \nL 205.989012 59.979243 \nL 207.097048 59.979243 \nL 220.39348 68.697081 \nL 221.501516 70.150054 \nL 222.609552 70.876541 \nL 223.717588 72.329514 \nL 224.825624 73.056 \nL 225.93366 75.235459 \nL 227.041696 75.961946 \nL 228.149732 77.414919 \nL 229.257768 78.141405 \nL 231.47384 81.047351 \nL 232.581876 81.773838 \nL 233.689912 83.226811 \nL 237.01402 85.40627 \nL 238.122056 86.859243 \nL 240.338128 88.312216 \nL 241.446164 89.765189 \nL 242.5542 90.491676 \nL 244.770272 93.397622 \nL 245.878308 96.303568 \nL 246.986344 97.030054 \nL 248.09438 100.662486 \nL 249.202416 101.388973 \nL 250.310452 101.388973 \nL 252.526524 104.294919 \nL 254.742596 105.747892 \nL 255.850632 107.927351 \nL 256.958668 107.927351 \nL 259.17474 109.380324 \nL 260.282776 109.380324 \nL 262.498848 112.28627 \nL 263.606884 113.012757 \nL 264.71492 114.46573 \nL 265.822956 115.192216 \nL 266.930992 116.645189 \nL 268.039028 115.192216 \nL 269.147064 114.46573 \nL 270.2551 115.918703 \nL 271.363136 116.645189 \nL 272.471172 116.645189 \nL 273.579208 120.277622 \nL 278.011352 126.089514 \nL 279.119388 126.816 \nL 280.227424 124.636541 \nL 281.33546 124.636541 \nL 282.443496 128.268973 \nL 283.551532 126.816 \nL 284.659568 126.816 \nL 285.767604 128.995459 \nL 286.87564 131.901405 \nL 287.983676 133.354378 \nL 289.091712 134.080865 \nL 290.199748 133.354378 \nL 291.307784 135.533838 \nL 292.41582 136.260324 \nL 293.523855 138.439784 \nL 294.631891 139.16627 \nL 295.739927 140.619243 \nL 297.955999 142.072216 \nL 299.064035 144.251676 \nL 302.388143 146.431135 \nL 303.496179 147.884108 \nL 304.604215 150.063568 \nL 306.820287 152.969514 \nL 307.928323 155.148973 \nL 310.144395 158.054919 \nL 312.360467 162.413838 \nL 317.900647 169.678703 \nL 319.008683 171.858162 \nL 321.224755 174.764108 \nL 322.332791 174.764108 \nL 323.440827 175.490595 \nL 324.548863 178.396541 \nL 326.764935 182.755459 \nL 327.872971 184.208432 \nL 328.981007 186.387892 \nL 331.197079 187.840865 \nL 332.305115 187.840865 \nL 333.413151 187.114378 \nL 334.521187 190.746811 \nL 335.629223 191.473297 \nL 336.737259 192.92627 \nL 337.845295 198.011676 \nL 338.953331 201.644108 \nL 341.169403 204.550054 \nL 342.277439 205.276541 \nL 343.385475 208.908973 \nL 344.493511 209.635459 \nL 345.601547 211.814919 \nL 346.709583 219.079784 \nL 347.817619 220.532757 \nL 348.925655 222.712216 \nL 350.033691 223.438703 \nL 351.141727 223.438703 \nL 352.249763 222.712216 \nL 353.357799 225.618162 \nL 354.465835 236.515459 \nL 355.573871 237.968432 \nL 356.681907 240.147892 \nL 357.789943 241.600865 \nL 360.006015 240.147892 \nL 360.006015 240.147892 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #ff7f0e; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path d=\"M 66.376477 23.654919 \nL 67.484513 25.107892 \nL 68.592549 25.107892 \nL 69.700585 25.834378 \nL 70.808621 25.834378 \nL 71.916657 26.560865 \nL 73.024693 25.834378 \nL 74.132729 23.654919 \nL 75.240765 22.928432 \nL 76.348801 23.654919 \nL 78.564873 23.654919 \nL 79.672909 24.381405 \nL 81.888981 24.381405 \nL 84.105053 25.834378 \nL 85.213089 25.834378 \nL 86.321125 26.560865 \nL 87.429161 25.834378 \nL 88.537197 22.201946 \nL 89.645233 22.201946 \nL 90.753269 25.107892 \nL 95.185413 28.013838 \nL 96.293449 28.013838 \nL 98.509521 29.466811 \nL 99.617557 29.466811 \nL 100.725593 28.740324 \nL 101.833629 29.466811 \nL 102.941665 28.740324 \nL 105.157737 30.193297 \nL 106.265773 30.193297 \nL 108.481845 31.64627 \nL 109.589881 33.82573 \nL 110.697917 33.82573 \nL 111.805953 30.919784 \nL 112.913989 30.193297 \nL 114.022025 31.64627 \nL 115.130061 30.193297 \nL 116.238097 31.64627 \nL 117.346133 32.372757 \nL 118.454169 33.82573 \nL 119.562205 33.82573 \nL 121.778277 35.278703 \nL 122.886313 33.82573 \nL 125.102385 32.372757 \nL 126.210421 33.099243 \nL 127.318457 33.099243 \nL 128.426493 33.82573 \nL 129.534529 33.82573 \nL 132.858637 36.005189 \nL 133.966673 36.005189 \nL 135.074709 36.731676 \nL 139.506853 36.731676 \nL 141.722925 38.184649 \nL 142.830961 39.637622 \nL 145.047033 41.090595 \nL 146.155069 41.090595 \nL 147.263105 38.911135 \nL 148.371141 38.184649 \nL 149.479177 38.911135 \nL 150.587213 38.911135 \nL 151.695249 40.364108 \nL 152.803285 41.090595 \nL 153.911321 41.090595 \nL 159.451501 44.723027 \nL 161.667573 38.911135 \nL 162.775609 38.184649 \nL 163.883645 40.364108 \nL 167.207752 42.543568 \nL 168.315788 42.543568 \nL 169.423824 43.270054 \nL 170.53186 44.723027 \nL 171.639896 43.996541 \nL 172.747932 44.723027 \nL 174.964004 44.723027 \nL 176.07204 46.902486 \nL 177.180076 47.628973 \nL 178.288112 47.628973 \nL 182.720256 50.534919 \nL 183.828292 50.534919 \nL 184.936328 51.261405 \nL 186.044364 51.261405 \nL 187.1524 51.987892 \nL 188.260436 51.987892 \nL 191.584544 54.167351 \nL 192.69258 54.167351 \nL 196.016688 56.346811 \nL 198.23276 56.346811 \nL 200.448832 57.799784 \nL 201.556868 57.799784 \nL 202.664904 58.52627 \nL 203.77294 58.52627 \nL 204.880976 59.979243 \nL 205.989012 60.70573 \nL 207.097048 62.158703 \nL 208.205084 62.158703 \nL 210.421156 63.611676 \nL 211.529192 65.064649 \nL 221.501516 71.603027 \nL 222.609552 73.056 \nL 225.93366 75.235459 \nL 229.257768 79.594378 \nL 231.47384 81.047351 \nL 232.581876 83.953297 \nL 233.689912 85.40627 \nL 235.905984 86.859243 \nL 239.230092 91.218162 \nL 240.338128 91.944649 \nL 241.446164 93.397622 \nL 243.662236 94.850595 \nL 244.770272 96.303568 \nL 245.878308 97.030054 \nL 246.986344 98.483027 \nL 248.09438 99.209514 \nL 249.202416 99.209514 \nL 250.310452 100.662486 \nL 251.418488 105.747892 \nL 252.526524 107.200865 \nL 253.63456 107.927351 \nL 254.742596 110.106811 \nL 256.958668 111.559784 \nL 258.066704 111.559784 \nL 260.282776 113.012757 \nL 262.498848 115.918703 \nL 263.606884 116.645189 \nL 264.71492 118.098162 \nL 265.822956 118.824649 \nL 266.930992 121.730595 \nL 268.039028 122.457081 \nL 270.2551 122.457081 \nL 271.363136 123.910054 \nL 272.471172 127.542486 \nL 273.579208 128.268973 \nL 275.79528 131.174919 \nL 276.903316 129.721946 \nL 278.011352 129.721946 \nL 279.119388 130.448432 \nL 280.227424 131.901405 \nL 281.33546 131.901405 \nL 282.443496 133.354378 \nL 283.551532 136.260324 \nL 286.87564 140.619243 \nL 287.983676 142.798703 \nL 289.091712 144.251676 \nL 290.199748 141.34573 \nL 291.307784 140.619243 \nL 293.523855 144.978162 \nL 294.631891 146.431135 \nL 296.847963 146.431135 \nL 297.955999 148.610595 \nL 299.064035 152.243027 \nL 302.388143 158.781405 \nL 303.496179 159.507892 \nL 304.604215 158.054919 \nL 305.712251 158.054919 \nL 307.928323 160.960865 \nL 309.036359 163.140324 \nL 311.252431 166.04627 \nL 312.360467 168.22573 \nL 313.468503 169.678703 \nL 314.576539 171.858162 \nL 317.900647 176.217081 \nL 319.008683 176.217081 \nL 320.116719 177.670054 \nL 321.224755 179.849514 \nL 322.332791 183.481946 \nL 323.440827 185.661405 \nL 325.656899 187.114378 \nL 326.764935 190.746811 \nL 327.872971 192.199784 \nL 328.981007 194.379243 \nL 330.089043 195.10573 \nL 331.197079 197.285189 \nL 332.305115 195.832216 \nL 333.413151 200.191135 \nL 334.521187 202.370595 \nL 335.629223 203.823568 \nL 336.737259 206.003027 \nL 337.845295 206.003027 \nL 340.061367 208.908973 \nL 341.169403 212.541405 \nL 342.277439 213.267892 \nL 345.601547 219.80627 \nL 346.709583 219.80627 \nL 348.925655 221.259243 \nL 351.141727 224.165189 \nL 352.249763 224.891676 \nL 353.357799 226.344649 \nL 354.465835 229.250595 \nL 356.681907 232.156541 \nL 357.789943 234.336 \nL 358.897979 235.788973 \nL 360.006015 236.515459 \nL 360.006015 236.515459 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #2ca02c; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path d=\"M 66.376477 27.287351 \nL 67.484513 27.287351 \nL 68.592549 28.013838 \nL 69.700585 28.013838 \nL 70.808621 28.740324 \nL 71.916657 28.740324 \nL 73.024693 25.834378 \nL 74.132729 24.381405 \nL 75.240765 25.107892 \nL 76.348801 25.107892 \nL 77.456837 25.834378 \nL 78.564873 25.834378 \nL 79.672909 26.560865 \nL 81.888981 26.560865 \nL 82.997017 27.287351 \nL 84.105053 27.287351 \nL 85.213089 28.013838 \nL 86.321125 27.287351 \nL 88.537197 27.287351 \nL 91.861305 29.466811 \nL 92.969341 29.466811 \nL 94.077377 30.193297 \nL 95.185413 30.193297 \nL 96.293449 30.919784 \nL 97.401485 30.193297 \nL 98.509521 30.919784 \nL 99.617557 30.193297 \nL 100.725593 30.193297 \nL 101.833629 30.919784 \nL 102.941665 30.919784 \nL 104.049701 31.64627 \nL 105.157737 33.099243 \nL 107.373809 34.552216 \nL 108.481845 34.552216 \nL 109.589881 32.372757 \nL 110.697917 30.919784 \nL 111.805953 31.64627 \nL 112.913989 31.64627 \nL 115.130061 33.099243 \nL 117.346133 33.099243 \nL 119.562205 34.552216 \nL 120.670241 34.552216 \nL 121.778277 33.82573 \nL 122.886313 31.64627 \nL 123.994349 31.64627 \nL 125.102385 32.372757 \nL 126.210421 31.64627 \nL 128.426493 33.099243 \nL 129.534529 33.099243 \nL 130.642565 33.82573 \nL 131.750601 33.82573 \nL 132.858637 34.552216 \nL 133.966673 34.552216 \nL 135.074709 35.278703 \nL 136.182745 33.82573 \nL 137.290781 34.552216 \nL 138.398817 33.82573 \nL 139.506853 33.82573 \nL 141.722925 35.278703 \nL 142.830961 36.731676 \nL 145.047033 38.184649 \nL 146.155069 37.458162 \nL 147.263105 34.552216 \nL 148.371141 34.552216 \nL 149.479177 35.278703 \nL 150.587213 35.278703 \nL 151.695249 36.731676 \nL 152.803285 36.731676 \nL 153.911321 37.458162 \nL 155.019357 37.458162 \nL 158.343465 39.637622 \nL 159.451501 39.637622 \nL 160.559537 37.458162 \nL 161.667573 33.099243 \nL 162.775609 33.099243 \nL 164.99168 36.005189 \nL 167.207752 37.458162 \nL 168.315788 37.458162 \nL 169.423824 38.184649 \nL 172.747932 38.184649 \nL 173.855968 38.911135 \nL 174.964004 40.364108 \nL 176.07204 40.364108 \nL 177.180076 41.090595 \nL 178.288112 41.090595 \nL 180.504184 42.543568 \nL 182.720256 42.543568 \nL 184.936328 43.996541 \nL 187.1524 43.996541 \nL 188.260436 44.723027 \nL 189.368472 44.723027 \nL 191.584544 46.176 \nL 192.69258 46.176 \nL 193.800616 46.902486 \nL 194.908652 46.902486 \nL 196.016688 47.628973 \nL 198.23276 47.628973 \nL 199.340796 48.355459 \nL 200.448832 48.355459 \nL 201.556868 49.081946 \nL 202.664904 49.081946 \nL 208.205084 52.714378 \nL 209.31312 52.714378 \nL 210.421156 54.167351 \nL 212.637228 55.620324 \nL 213.745264 55.620324 \nL 227.041696 64.338162 \nL 229.257768 67.244108 \nL 231.47384 68.697081 \nL 232.581876 70.150054 \nL 233.689912 70.876541 \nL 235.905984 73.782486 \nL 238.122056 75.235459 \nL 241.446164 79.594378 \nL 243.662236 81.047351 \nL 244.770272 83.226811 \nL 245.878308 83.953297 \nL 246.986344 85.40627 \nL 251.418488 88.312216 \nL 252.526524 92.671135 \nL 253.63456 99.209514 \nL 254.742596 100.662486 \nL 255.850632 104.294919 \nL 256.958668 103.568432 \nL 258.066704 104.294919 \nL 259.17474 105.747892 \nL 261.390812 105.747892 \nL 262.498848 107.200865 \nL 263.606884 109.380324 \nL 264.71492 110.833297 \nL 265.822956 111.559784 \nL 266.930992 113.012757 \nL 268.039028 113.012757 \nL 269.147064 111.559784 \nL 270.2551 111.559784 \nL 271.363136 113.012757 \nL 273.579208 113.012757 \nL 274.687244 115.918703 \nL 276.903316 118.824649 \nL 278.011352 121.004108 \nL 280.227424 123.910054 \nL 281.33546 123.910054 \nL 282.443496 121.004108 \nL 284.659568 122.457081 \nL 285.767604 124.636541 \nL 286.87564 125.363027 \nL 287.983676 123.910054 \nL 289.091712 124.636541 \nL 290.199748 128.268973 \nL 293.523855 134.807351 \nL 294.631891 136.260324 \nL 295.739927 134.080865 \nL 296.847963 132.627892 \nL 297.955999 133.354378 \nL 299.064035 136.260324 \nL 304.604215 143.525189 \nL 305.712251 144.251676 \nL 309.036359 148.610595 \nL 310.144395 152.969514 \nL 317.900647 168.22573 \nL 319.008683 171.858162 \nL 320.116719 172.584649 \nL 322.332791 178.396541 \nL 323.440827 179.849514 \nL 324.548863 182.028973 \nL 325.656899 183.481946 \nL 326.764935 184.208432 \nL 330.089043 192.92627 \nL 331.197079 195.10573 \nL 332.305115 198.738162 \nL 333.413151 199.464649 \nL 334.521187 200.917622 \nL 335.629223 203.097081 \nL 336.737259 202.370595 \nL 337.845295 205.276541 \nL 338.953331 207.456 \nL 340.061367 211.088432 \nL 341.169403 213.267892 \nL 342.277439 214.720865 \nL 343.385475 216.900324 \nL 344.493511 218.353297 \nL 345.601547 218.353297 \nL 346.709583 220.532757 \nL 348.925655 223.438703 \nL 350.033691 233.609514 \nL 352.249763 236.515459 \nL 353.357799 235.062486 \nL 354.465835 235.062486 \nL 355.573871 236.515459 \nL 356.681907 240.874378 \nL 357.789943 251.045189 \nL 358.897979 253.224649 \nL 360.006015 251.771676 \nL 360.006015 251.771676 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #d62728; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path d=\"M 66.376477 22.928432 \nL 67.484513 23.654919 \nL 68.592549 23.654919 \nL 70.808621 25.107892 \nL 71.916657 24.381405 \nL 73.024693 21.475459 \nL 74.132729 21.475459 \nL 75.240765 22.201946 \nL 76.348801 22.201946 \nL 77.456837 22.928432 \nL 79.672909 22.928432 \nL 81.888981 24.381405 \nL 82.997017 24.381405 \nL 84.105053 25.107892 \nL 85.213089 24.381405 \nL 86.321125 20.748973 \nL 87.429161 25.107892 \nL 92.969341 28.740324 \nL 94.077377 28.013838 \nL 95.185413 28.740324 \nL 96.293449 28.013838 \nL 97.401485 28.013838 \nL 98.509521 29.466811 \nL 99.617557 29.466811 \nL 100.725593 30.193297 \nL 101.833629 31.64627 \nL 104.049701 33.099243 \nL 105.157737 32.372757 \nL 106.265773 29.466811 \nL 108.481845 29.466811 \nL 109.589881 30.193297 \nL 110.697917 30.193297 \nL 114.022025 32.372757 \nL 115.130061 32.372757 \nL 116.238097 33.099243 \nL 117.346133 33.099243 \nL 118.454169 33.82573 \nL 119.562205 31.64627 \nL 121.778277 30.193297 \nL 122.886313 30.919784 \nL 123.994349 30.919784 \nL 125.102385 31.64627 \nL 126.210421 31.64627 \nL 129.534529 33.82573 \nL 136.182745 33.82573 \nL 137.290781 34.552216 \nL 138.398817 34.552216 \nL 139.506853 36.005189 \nL 140.614889 36.731676 \nL 141.722925 36.005189 \nL 142.830961 33.82573 \nL 143.938997 33.82573 \nL 145.047033 35.278703 \nL 147.263105 36.731676 \nL 148.371141 38.184649 \nL 150.587213 39.637622 \nL 151.695249 38.911135 \nL 152.803285 34.552216 \nL 153.911321 32.372757 \nL 155.019357 32.372757 \nL 156.127393 34.552216 \nL 157.235429 36.005189 \nL 159.451501 37.458162 \nL 160.559537 37.458162 \nL 161.667573 38.184649 \nL 162.775609 38.184649 \nL 163.883645 38.911135 \nL 164.99168 38.184649 \nL 170.53186 41.817081 \nL 171.639896 41.817081 \nL 173.855968 43.270054 \nL 174.964004 43.270054 \nL 176.07204 44.723027 \nL 178.288112 44.723027 \nL 179.396148 45.449514 \nL 180.504184 45.449514 \nL 184.936328 48.355459 \nL 186.044364 48.355459 \nL 187.1524 49.081946 \nL 189.368472 49.081946 \nL 190.476508 49.808432 \nL 192.69258 49.808432 \nL 193.800616 50.534919 \nL 194.908652 50.534919 \nL 203.77294 56.346811 \nL 204.880976 56.346811 \nL 205.989012 57.799784 \nL 207.097048 57.799784 \nL 210.421156 59.979243 \nL 211.529192 59.979243 \nL 212.637228 60.70573 \nL 213.745264 62.158703 \nL 214.8533 62.158703 \nL 215.961336 63.611676 \nL 219.285444 65.791135 \nL 220.39348 67.244108 \nL 221.501516 67.970595 \nL 222.609552 69.423568 \nL 223.717588 70.150054 \nL 224.825624 71.603027 \nL 225.93366 72.329514 \nL 228.149732 75.235459 \nL 230.365804 76.688432 \nL 233.689912 81.047351 \nL 234.797948 81.773838 \nL 235.905984 83.226811 \nL 237.01402 83.953297 \nL 238.122056 85.40627 \nL 240.338128 86.859243 \nL 241.446164 88.312216 \nL 242.5542 88.312216 \nL 244.770272 89.765189 \nL 245.878308 94.124108 \nL 248.09438 95.577081 \nL 249.202416 99.209514 \nL 251.418488 100.662486 \nL 252.526524 100.662486 \nL 253.63456 102.115459 \nL 254.742596 102.841946 \nL 255.850632 104.294919 \nL 258.066704 105.747892 \nL 259.17474 107.200865 \nL 260.282776 107.200865 \nL 263.606884 109.380324 \nL 264.71492 108.653838 \nL 265.822956 113.012757 \nL 266.930992 113.739243 \nL 268.039028 116.645189 \nL 269.147064 115.192216 \nL 270.2551 114.46573 \nL 271.363136 114.46573 \nL 272.471172 115.918703 \nL 273.579208 115.918703 \nL 274.687244 117.371676 \nL 275.79528 119.551135 \nL 276.903316 123.183568 \nL 278.011352 123.910054 \nL 279.119388 126.089514 \nL 280.227424 126.816 \nL 281.33546 124.636541 \nL 282.443496 123.183568 \nL 283.551532 123.910054 \nL 284.659568 126.089514 \nL 285.767604 127.542486 \nL 287.983676 126.089514 \nL 290.199748 131.901405 \nL 292.41582 136.260324 \nL 293.523855 137.713297 \nL 294.631891 136.986811 \nL 295.739927 135.533838 \nL 296.847963 135.533838 \nL 297.955999 137.713297 \nL 304.604215 146.431135 \nL 305.712251 151.516541 \nL 306.820287 151.516541 \nL 307.928323 152.969514 \nL 309.036359 155.875459 \nL 310.144395 156.601946 \nL 311.252431 158.781405 \nL 315.684575 164.593297 \nL 316.792611 166.772757 \nL 317.900647 169.678703 \nL 321.224755 174.037622 \nL 322.332791 176.943568 \nL 323.440827 177.670054 \nL 324.548863 177.670054 \nL 325.656899 179.123027 \nL 326.764935 182.755459 \nL 328.981007 187.114378 \nL 330.089043 188.567351 \nL 331.197079 190.746811 \nL 332.305115 192.199784 \nL 334.521187 193.652757 \nL 335.629223 195.10573 \nL 336.737259 198.011676 \nL 337.845295 198.011676 \nL 338.953331 200.191135 \nL 340.061367 203.823568 \nL 341.169403 206.729514 \nL 342.277439 208.908973 \nL 343.385475 210.361946 \nL 344.493511 213.267892 \nL 345.601547 213.267892 \nL 346.709583 216.173838 \nL 347.817619 217.626811 \nL 348.925655 219.80627 \nL 350.033691 221.259243 \nL 351.141727 223.438703 \nL 354.465835 227.797622 \nL 355.573871 229.977081 \nL 357.789943 232.883027 \nL 358.897979 232.156541 \nL 358.897979 232.156541 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #9467bd; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path d=\"M 66.376477 23.654919 \nL 67.484513 24.381405 \nL 68.592549 24.381405 \nL 70.808621 25.834378 \nL 71.916657 25.834378 \nL 73.024693 22.201946 \nL 74.132729 21.475459 \nL 76.348801 22.928432 \nL 77.456837 22.201946 \nL 79.672909 23.654919 \nL 81.888981 23.654919 \nL 82.997017 24.381405 \nL 84.105053 24.381405 \nL 85.213089 25.834378 \nL 86.321125 24.381405 \nL 87.429161 21.475459 \nL 88.537197 24.381405 \nL 89.645233 25.107892 \nL 90.753269 26.560865 \nL 92.969341 28.013838 \nL 94.077377 28.013838 \nL 95.185413 28.740324 \nL 97.401485 27.287351 \nL 98.509521 28.740324 \nL 99.617557 29.466811 \nL 100.725593 31.64627 \nL 101.833629 31.64627 \nL 102.941665 32.372757 \nL 104.049701 29.466811 \nL 105.157737 28.013838 \nL 106.265773 28.740324 \nL 107.373809 28.740324 \nL 110.697917 30.919784 \nL 112.913989 30.919784 \nL 115.130061 32.372757 \nL 116.238097 30.919784 \nL 117.346133 28.740324 \nL 118.454169 29.466811 \nL 119.562205 28.740324 \nL 120.670241 29.466811 \nL 121.778277 29.466811 \nL 123.994349 30.919784 \nL 125.102385 30.919784 \nL 126.210421 31.64627 \nL 131.750601 31.64627 \nL 132.858637 30.919784 \nL 133.966673 31.64627 \nL 136.182745 31.64627 \nL 137.290781 30.919784 \nL 138.398817 31.64627 \nL 139.506853 31.64627 \nL 140.614889 33.099243 \nL 143.938997 35.278703 \nL 145.047033 35.278703 \nL 147.263105 36.731676 \nL 148.371141 34.552216 \nL 149.479177 29.466811 \nL 150.587213 28.740324 \nL 151.695249 28.740324 \nL 152.803285 30.919784 \nL 158.343465 34.552216 \nL 159.451501 34.552216 \nL 160.559537 35.278703 \nL 162.775609 35.278703 \nL 163.883645 36.731676 \nL 164.99168 36.731676 \nL 167.207752 38.184649 \nL 168.315788 38.184649 \nL 171.639896 40.364108 \nL 172.747932 40.364108 \nL 173.855968 41.090595 \nL 174.964004 41.090595 \nL 176.07204 41.817081 \nL 177.180076 41.817081 \nL 178.288112 42.543568 \nL 179.396148 42.543568 \nL 181.61222 43.996541 \nL 182.720256 43.996541 \nL 184.936328 45.449514 \nL 187.1524 45.449514 \nL 188.260436 46.176 \nL 189.368472 46.176 \nL 190.476508 46.902486 \nL 191.584544 46.902486 \nL 193.800616 48.355459 \nL 194.908652 48.355459 \nL 196.016688 50.534919 \nL 204.880976 56.346811 \nL 205.989012 58.52627 \nL 207.097048 59.252757 \nL 208.205084 60.70573 \nL 209.31312 61.432216 \nL 210.421156 63.611676 \nL 212.637228 65.064649 \nL 215.961336 69.423568 \nL 217.069372 70.150054 \nL 220.39348 74.508973 \nL 221.501516 75.235459 \nL 222.609552 76.688432 \nL 223.717588 77.414919 \nL 224.825624 79.594378 \nL 225.93366 81.047351 \nL 227.041696 81.773838 \nL 228.149732 83.226811 \nL 229.257768 83.953297 \nL 231.47384 86.859243 \nL 233.689912 88.312216 \nL 234.797948 89.765189 \nL 237.01402 91.218162 \nL 238.122056 96.303568 \nL 239.230092 97.756541 \nL 240.338128 98.483027 \nL 241.446164 102.115459 \nL 242.5542 102.115459 \nL 243.662236 103.568432 \nL 245.878308 105.021405 \nL 246.986344 106.474378 \nL 248.09438 107.200865 \nL 249.202416 108.653838 \nL 253.63456 111.559784 \nL 254.742596 110.833297 \nL 255.850632 110.833297 \nL 256.958668 111.559784 \nL 258.066704 113.739243 \nL 259.17474 115.192216 \nL 260.282776 115.918703 \nL 262.498848 118.824649 \nL 263.606884 116.645189 \nL 268.039028 119.551135 \nL 269.147064 121.004108 \nL 270.2551 126.089514 \nL 273.579208 130.448432 \nL 274.687244 126.816 \nL 275.79528 126.089514 \nL 276.903316 126.816 \nL 278.011352 128.995459 \nL 279.119388 130.448432 \nL 280.227424 128.995459 \nL 281.33546 128.995459 \nL 282.443496 130.448432 \nL 283.551532 136.260324 \nL 284.659568 138.439784 \nL 285.767604 139.16627 \nL 286.87564 136.260324 \nL 287.983676 136.260324 \nL 289.091712 136.986811 \nL 290.199748 139.16627 \nL 293.523855 141.34573 \nL 294.631891 142.798703 \nL 295.739927 143.525189 \nL 296.847963 144.978162 \nL 299.064035 146.431135 \nL 301.280107 146.431135 \nL 302.388143 148.610595 \nL 303.496179 149.337081 \nL 305.712251 153.696 \nL 306.820287 154.422486 \nL 312.360467 161.687351 \nL 313.468503 162.413838 \nL 314.576539 163.866811 \nL 316.792611 165.319784 \nL 317.900647 166.772757 \nL 319.008683 167.499243 \nL 320.116719 167.499243 \nL 321.224755 168.22573 \nL 322.332791 174.037622 \nL 323.440827 175.490595 \nL 324.548863 177.670054 \nL 325.656899 178.396541 \nL 326.764935 179.849514 \nL 327.872971 179.123027 \nL 328.981007 179.123027 \nL 330.089043 182.028973 \nL 331.197079 182.028973 \nL 332.305115 183.481946 \nL 333.413151 186.387892 \nL 334.521187 188.567351 \nL 336.737259 191.473297 \nL 337.845295 193.652757 \nL 338.953331 195.10573 \nL 340.061367 198.011676 \nL 342.277439 200.917622 \nL 343.385475 203.823568 \nL 344.493511 204.550054 \nL 345.601547 206.729514 \nL 347.817619 209.635459 \nL 350.033691 213.994378 \nL 352.249763 212.541405 \nL 353.357799 213.267892 \nL 354.465835 215.447351 \nL 355.573871 229.250595 \nL 356.681907 230.703568 \nL 357.789943 232.883027 \nL 358.897979 233.609514 \nL 360.006015 232.156541 \nL 360.006015 232.156541 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #8c564b; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_21\">\n    <path d=\"M 66.376477 20.748973 \nL 67.484513 21.475459 \nL 68.592549 21.475459 \nL 70.808621 22.928432 \nL 71.916657 20.748973 \nL 73.024693 19.296 \nL 74.132729 20.022486 \nL 75.240765 20.022486 \nL 77.456837 21.475459 \nL 78.564873 21.475459 \nL 79.672909 22.201946 \nL 81.888981 22.201946 \nL 84.105053 23.654919 \nL 85.213089 23.654919 \nL 86.321125 22.928432 \nL 87.429161 21.475459 \nL 88.537197 23.654919 \nL 89.645233 24.381405 \nL 90.753269 25.834378 \nL 91.861305 26.560865 \nL 92.969341 25.834378 \nL 94.077377 27.287351 \nL 95.185413 28.013838 \nL 97.401485 28.013838 \nL 98.509521 27.287351 \nL 100.725593 30.193297 \nL 105.157737 33.099243 \nL 106.265773 30.919784 \nL 107.373809 29.466811 \nL 108.481845 30.193297 \nL 109.589881 30.193297 \nL 114.022025 33.099243 \nL 115.130061 33.099243 \nL 116.238097 33.82573 \nL 118.454169 33.82573 \nL 119.562205 31.64627 \nL 120.670241 31.64627 \nL 121.778277 32.372757 \nL 123.994349 32.372757 \nL 125.102385 33.099243 \nL 126.210421 33.099243 \nL 129.534529 35.278703 \nL 130.642565 35.278703 \nL 131.750601 36.005189 \nL 132.858637 36.005189 \nL 133.966673 35.278703 \nL 135.074709 36.005189 \nL 136.182745 35.278703 \nL 139.506853 37.458162 \nL 140.614889 38.911135 \nL 141.722925 39.637622 \nL 142.830961 37.458162 \nL 143.938997 36.005189 \nL 145.047033 36.731676 \nL 146.155069 36.731676 \nL 147.263105 38.911135 \nL 148.371141 40.364108 \nL 149.479177 41.090595 \nL 150.587213 41.090595 \nL 151.695249 42.543568 \nL 152.803285 38.911135 \nL 153.911321 36.005189 \nL 155.019357 35.278703 \nL 156.127393 36.005189 \nL 157.235429 37.458162 \nL 162.775609 41.090595 \nL 163.883645 41.090595 \nL 164.99168 41.817081 \nL 166.099716 41.817081 \nL 167.207752 41.090595 \nL 168.315788 41.090595 \nL 169.423824 41.817081 \nL 170.53186 41.817081 \nL 171.639896 43.270054 \nL 174.964004 45.449514 \nL 176.07204 45.449514 \nL 177.180076 46.176 \nL 178.288112 46.176 \nL 180.504184 47.628973 \nL 181.61222 47.628973 \nL 182.720256 48.355459 \nL 184.936328 48.355459 \nL 188.260436 50.534919 \nL 189.368472 50.534919 \nL 190.476508 51.261405 \nL 191.584544 51.261405 \nL 192.69258 51.987892 \nL 193.800616 51.987892 \nL 194.908652 52.714378 \nL 196.016688 52.714378 \nL 197.124724 53.440865 \nL 198.23276 53.440865 \nL 199.340796 54.893838 \nL 203.77294 57.799784 \nL 204.880976 57.799784 \nL 208.205084 59.979243 \nL 209.31312 59.979243 \nL 213.745264 62.885189 \nL 214.8533 62.885189 \nL 218.177408 65.064649 \nL 219.285444 66.517622 \nL 222.609552 68.697081 \nL 223.717588 72.329514 \nL 225.93366 73.782486 \nL 228.149732 76.688432 \nL 230.365804 78.141405 \nL 233.689912 82.500324 \nL 237.01402 84.679784 \nL 238.122056 86.132757 \nL 240.338128 87.58573 \nL 241.446164 89.038703 \nL 242.5542 89.038703 \nL 243.662236 90.491676 \nL 244.770272 91.218162 \nL 245.878308 91.218162 \nL 246.986344 91.944649 \nL 248.09438 100.662486 \nL 249.202416 101.388973 \nL 250.310452 104.294919 \nL 251.418488 105.747892 \nL 253.63456 107.200865 \nL 254.742596 107.200865 \nL 258.066704 109.380324 \nL 260.282776 109.380324 \nL 262.498848 112.28627 \nL 263.606884 113.012757 \nL 265.822956 115.918703 \nL 268.039028 114.46573 \nL 270.2551 117.371676 \nL 271.363136 119.551135 \nL 273.579208 122.457081 \nL 274.687244 124.636541 \nL 276.903316 127.542486 \nL 278.011352 127.542486 \nL 279.119388 124.636541 \nL 280.227424 123.910054 \nL 282.443496 128.268973 \nL 283.551532 128.995459 \nL 284.659568 128.268973 \nL 285.767604 128.995459 \nL 286.87564 132.627892 \nL 287.983676 134.807351 \nL 291.307784 139.16627 \nL 292.41582 136.260324 \nL 294.631891 136.260324 \nL 295.739927 139.16627 \nL 296.847963 139.16627 \nL 297.955999 141.34573 \nL 300.172071 144.251676 \nL 301.280107 144.978162 \nL 302.388143 146.431135 \nL 303.496179 150.063568 \nL 304.604215 150.063568 \nL 305.712251 153.696 \nL 306.820287 156.601946 \nL 307.928323 160.234378 \nL 309.036359 162.413838 \nL 311.252431 165.319784 \nL 312.360467 167.499243 \nL 317.900647 174.764108 \nL 319.008683 176.943568 \nL 320.116719 178.396541 \nL 321.224755 179.123027 \nL 322.332791 183.481946 \nL 324.548863 187.840865 \nL 325.656899 189.293838 \nL 328.981007 195.832216 \nL 331.197079 197.285189 \nL 332.305115 198.738162 \nL 333.413151 198.738162 \nL 334.521187 200.191135 \nL 335.629223 203.823568 \nL 336.737259 206.003027 \nL 338.953331 208.908973 \nL 340.061367 211.088432 \nL 342.277439 211.088432 \nL 343.385475 213.267892 \nL 344.493511 214.720865 \nL 345.601547 222.712216 \nL 346.709583 226.344649 \nL 347.817619 227.797622 \nL 348.925655 225.618162 \nL 350.033691 225.618162 \nL 351.141727 227.797622 \nL 352.249763 240.147892 \nL 353.357799 240.874378 \nL 354.465835 243.053838 \nL 355.573871 244.506811 \nL 356.681907 243.780324 \nL 356.681907 243.780324 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #e377c2; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"line2d_22\">\n    <path d=\"M 66.376477 30.919784 \nL 68.592549 32.372757 \nL 70.808621 32.372757 \nL 71.916657 33.099243 \nL 73.024693 33.099243 \nL 74.132729 29.466811 \nL 75.240765 28.740324 \nL 76.348801 29.466811 \nL 77.456837 29.466811 \nL 78.564873 30.193297 \nL 82.997017 30.193297 \nL 85.213089 31.64627 \nL 86.321125 30.193297 \nL 89.645233 32.372757 \nL 91.861305 32.372757 \nL 92.969341 33.099243 \nL 94.077377 33.099243 \nL 95.185413 33.82573 \nL 96.293449 33.099243 \nL 99.617557 33.099243 \nL 100.725593 32.372757 \nL 101.833629 32.372757 \nL 102.941665 33.82573 \nL 104.049701 33.099243 \nL 105.157737 33.82573 \nL 106.265773 35.278703 \nL 107.373809 36.005189 \nL 111.805953 36.005189 \nL 114.022025 33.099243 \nL 115.130061 33.099243 \nL 116.238097 33.82573 \nL 117.346133 33.82573 \nL 118.454169 34.552216 \nL 119.562205 34.552216 \nL 121.778277 36.005189 \nL 123.994349 36.005189 \nL 125.102385 36.731676 \nL 126.210421 36.005189 \nL 127.318457 36.005189 \nL 128.426493 35.278703 \nL 129.534529 35.278703 \nL 131.750601 36.731676 \nL 133.966673 36.731676 \nL 136.182745 38.184649 \nL 137.290781 37.458162 \nL 138.398817 35.278703 \nL 139.506853 35.278703 \nL 140.614889 36.005189 \nL 141.722925 36.005189 \nL 142.830961 38.184649 \nL 143.938997 38.184649 \nL 145.047033 39.637622 \nL 146.155069 39.637622 \nL 147.263105 40.364108 \nL 148.371141 39.637622 \nL 149.479177 36.005189 \nL 150.587213 33.82573 \nL 151.695249 35.278703 \nL 157.235429 38.911135 \nL 158.343465 38.911135 \nL 160.559537 40.364108 \nL 162.775609 38.911135 \nL 163.883645 39.637622 \nL 164.99168 39.637622 \nL 166.099716 41.090595 \nL 168.315788 42.543568 \nL 169.423824 42.543568 \nL 170.53186 43.270054 \nL 171.639896 43.270054 \nL 173.855968 44.723027 \nL 174.964004 44.723027 \nL 176.07204 45.449514 \nL 177.180076 45.449514 \nL 178.288112 46.176 \nL 179.396148 46.176 \nL 180.504184 46.902486 \nL 181.61222 46.902486 \nL 182.720256 47.628973 \nL 183.828292 47.628973 \nL 186.044364 49.081946 \nL 187.1524 49.081946 \nL 188.260436 49.808432 \nL 189.368472 49.808432 \nL 190.476508 51.261405 \nL 191.584544 51.261405 \nL 196.016688 54.167351 \nL 197.124724 55.620324 \nL 198.23276 55.620324 \nL 199.340796 56.346811 \nL 200.448832 56.346811 \nL 201.556868 57.799784 \nL 203.77294 59.252757 \nL 204.880976 59.252757 \nL 205.989012 59.979243 \nL 207.097048 59.979243 \nL 212.637228 63.611676 \nL 213.745264 65.791135 \nL 215.961336 67.244108 \nL 217.069372 68.697081 \nL 218.177408 69.423568 \nL 219.285444 70.876541 \nL 220.39348 71.603027 \nL 221.501516 73.056 \nL 222.609552 73.782486 \nL 224.825624 76.688432 \nL 225.93366 76.688432 \nL 230.365804 82.500324 \nL 231.47384 82.500324 \nL 232.581876 83.953297 \nL 235.905984 86.132757 \nL 237.01402 87.58573 \nL 240.338128 89.765189 \nL 241.446164 89.765189 \nL 242.5542 90.491676 \nL 243.662236 91.944649 \nL 244.770272 91.944649 \nL 245.878308 92.671135 \nL 246.986344 94.850595 \nL 248.09438 99.209514 \nL 253.63456 102.841946 \nL 254.742596 104.294919 \nL 255.850632 104.294919 \nL 256.958668 105.021405 \nL 258.066704 106.474378 \nL 260.282776 107.927351 \nL 261.390812 107.200865 \nL 262.498848 107.927351 \nL 263.606884 109.380324 \nL 264.71492 110.106811 \nL 265.822956 111.559784 \nL 266.930992 112.28627 \nL 268.039028 112.28627 \nL 269.147064 111.559784 \nL 270.2551 113.012757 \nL 272.471172 120.277622 \nL 273.579208 121.730595 \nL 274.687244 122.457081 \nL 275.79528 121.730595 \nL 276.903316 118.824649 \nL 278.011352 118.824649 \nL 279.119388 121.004108 \nL 280.227424 122.457081 \nL 281.33546 123.183568 \nL 282.443496 122.457081 \nL 283.551532 126.816 \nL 286.87564 131.174919 \nL 287.983676 131.901405 \nL 289.091712 128.268973 \nL 290.199748 127.542486 \nL 291.307784 128.268973 \nL 292.41582 129.721946 \nL 293.523855 130.448432 \nL 294.631891 131.901405 \nL 301.280107 136.260324 \nL 302.388143 135.533838 \nL 303.496179 135.533838 \nL 304.604215 138.439784 \nL 305.712251 138.439784 \nL 306.820287 139.892757 \nL 307.928323 142.798703 \nL 311.252431 144.978162 \nL 312.360467 147.157622 \nL 313.468503 148.610595 \nL 314.576539 148.610595 \nL 316.792611 150.063568 \nL 317.900647 151.516541 \nL 319.008683 152.243027 \nL 320.116719 151.516541 \nL 322.332791 151.516541 \nL 323.440827 155.148973 \nL 324.548863 159.507892 \nL 327.872971 159.507892 \nL 328.981007 160.960865 \nL 330.089043 160.960865 \nL 331.197079 164.593297 \nL 333.413151 167.499243 \nL 334.521187 167.499243 \nL 335.629223 166.772757 \nL 336.737259 168.952216 \nL 337.845295 174.037622 \nL 338.953331 174.764108 \nL 340.061367 176.217081 \nL 341.169403 176.943568 \nL 342.277439 178.396541 \nL 343.385475 179.123027 \nL 344.493511 176.217081 \nL 345.601547 175.490595 \nL 346.709583 176.943568 \nL 347.817619 188.567351 \nL 348.925655 190.020324 \nL 350.033691 190.746811 \nL 351.141727 190.746811 \nL 353.357799 189.293838 \nL 354.465835 191.473297 \nL 355.573871 192.199784 \nL 356.681907 194.379243 \nL 357.789943 195.10573 \nL 358.897979 198.738162 \nL 360.006015 200.917622 \nL 361.114051 203.823568 \nL 362.222087 206.003027 \nL 363.330123 207.456 \nL 364.438159 209.635459 \nL 366.654231 209.635459 \nL 368.870303 212.541405 \nL 369.978339 216.173838 \nL 371.086375 218.353297 \nL 373.302447 224.165189 \nL 374.410483 226.344649 \nL 376.626555 232.156541 \nL 377.734591 234.336 \nL 378.842627 235.788973 \nL 379.950663 234.336 \nL 381.058699 238.694919 \nL 383.274771 244.506811 \nL 384.382807 245.233297 \nL 385.490843 250.318703 \nL 386.598879 253.224649 \nL 387.706915 255.404108 \nL 388.814951 258.310054 \nL 389.922987 260.489514 \nL 391.031023 261.216 \nL 391.031023 261.216 \n\" clip-path=\"url(#p4f42b03115)\" style=\"fill: none; stroke: #7f7f7f; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 273.312 \nL 50.14375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 407.26375 273.312 \nL 407.26375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 273.312 \nL 407.26375 273.312 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 407.26375 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4f42b03115\">\n   <rect x=\"50.14375\" y=\"7.2\" width=\"357.12\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:39:47.027365Z",
     "start_time": "2025-03-11T02:39:46.985785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def setup_seed(seed):\n",
    "    \"\"\"set random seed\"\"\"\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    random.seed(seed)  # Python random module.\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed) \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def evaluation(y_test, y_predict):\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    rmse = sqrt(mean_squared_error(y_test, y_predict))\n",
    "    return rmse\n",
    "def get_data():\n",
    "    \"\"\"获取训练集，测试集，验证集\"\"\"\n",
    "    train_list = Battery_list\n",
    "    train_data=[]\n",
    "    for b_n in train_list:\n",
    "        path=data_root+b_n+'.npz'\n",
    "        arrays = np.load(path)\n",
    "        a,b=clean_data(arrays['array1'],arrays['array2'])\n",
    "        a=add_row_index_to_array(a)\n",
    "        train_data.append([a,b])\n",
    "    train_features=np.concatenate((train_data[0][0],train_data[1][0],train_data[2][0],train_data[3][0],train_data[4][0]),axis=0)\n",
    "    train_labls=np.concatenate((train_data[0][1],train_data[1][1],train_data[2][1],train_data[3][1],train_data[4][1]),axis=0)\n",
    "    train_valid_features=torch.from_numpy(train_features).float()\n",
    "    train_valid_labels=torch.from_numpy(train_labls).float()\n",
    "    dataset=TensorDataset(train_valid_features,train_valid_labels)\n",
    "    # 确定训练集和验证集的大小\n",
    "    train_size = int(0.6 * len(dataset))  # 80%的训练集\n",
    "    val_size = len(dataset) - train_size   # 剩余的20%作为验证集\n",
    "    # 随机分割数据集\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    # 创建DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    test_5_data,test_6_data,test_7_data=[train_data[5][0],train_data[5][1]],[train_data[6][0],train_data[6][1]],[train_data[7][0],train_data[7][1]]\n",
    "    return train_loader, val_loader, test_5_data,test_6_data,test_7_data\n",
    "\n",
    "a,b,c,d,e=get_data()\n",
    "print(d,e)\n",
    "for x,y in b:\n",
    "    print(x.shape)"
   ],
   "id": "4a28488cf5746bdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.00000000e+00, 8.10116599e-01, 2.05529220e-01, ...,\n",
      "        7.50713571e-01, 1.00000000e+00, 5.46999916e-03],\n",
      "       [1.00000000e+00, 8.10116599e-01, 2.05529220e-01, ...,\n",
      "        7.50713571e-01, 1.00000000e+00, 5.46999916e-03],\n",
      "       [2.00000000e+00, 8.10116599e-01, 2.05529220e-01, ...,\n",
      "        7.50713571e-01, 1.00000000e+00, 5.46999916e-03],\n",
      "       ...,\n",
      "       [2.60000000e+02, 4.56911290e-01, 8.30281021e-01, ...,\n",
      "        7.14768346e-01, 1.10282954e-01, 5.22973996e-01],\n",
      "       [2.61000000e+02, 4.64166178e-01, 7.90670802e-01, ...,\n",
      "        7.24159078e-01, 1.04209800e-01, 5.26676765e-01],\n",
      "       [2.62000000e+02, 4.68563913e-01, 7.76807393e-01, ...,\n",
      "        7.15860345e-01, 9.82746722e-02, 5.29832534e-01]]), array([1.967, 1.966, 1.966, 1.965, 1.964, 1.967, 1.969, 1.968, 1.968,\n",
      "       1.967, 1.966, 1.966, 1.965, 1.965, 1.965, 1.964, 1.963, 1.963,\n",
      "       1.964, 1.966, 1.963, 1.962, 1.96 , 1.959, 1.96 , 1.958, 1.957,\n",
      "       1.957, 1.957, 1.958, 1.956, 1.954, 1.953, 1.952, 1.951, 1.95 ,\n",
      "       1.953, 1.955, 1.954, 1.954, 1.953, 1.952, 1.951, 1.95 , 1.95 ,\n",
      "       1.949, 1.949, 1.949, 1.952, 1.952, 1.951, 1.951, 1.951, 1.95 ,\n",
      "       1.95 , 1.949, 1.948, 1.947, 1.947, 1.946, 1.946, 1.947, 1.946,\n",
      "       1.947, 1.946, 1.945, 1.944, 1.942, 1.941, 1.944, 1.946, 1.945,\n",
      "       1.945, 1.942, 1.94 , 1.939, 1.939, 1.937, 1.942, 1.946, 1.947,\n",
      "       1.946, 1.944, 1.943, 1.942, 1.941, 1.94 , 1.939, 1.939, 1.938,\n",
      "       1.938, 1.939, 1.939, 1.938, 1.938, 1.936, 1.935, 1.934, 1.933,\n",
      "       1.933, 1.932, 1.932, 1.931, 1.93 , 1.93 , 1.929, 1.929, 1.929,\n",
      "       1.928, 1.927, 1.926, 1.926, 1.925, 1.925, 1.924, 1.924, 1.923,\n",
      "       1.923, 1.922, 1.922, 1.92 , 1.919, 1.918, 1.917, 1.916, 1.916,\n",
      "       1.915, 1.914, 1.913, 1.913, 1.912, 1.911, 1.91 , 1.909, 1.909,\n",
      "       1.908, 1.907, 1.906, 1.904, 1.903, 1.902, 1.901, 1.896, 1.895,\n",
      "       1.894, 1.892, 1.89 , 1.889, 1.888, 1.886, 1.884, 1.882, 1.881,\n",
      "       1.88 , 1.879, 1.877, 1.876, 1.875, 1.873, 1.873, 1.871, 1.87 ,\n",
      "       1.87 , 1.869, 1.857, 1.856, 1.852, 1.85 , 1.849, 1.848, 1.848,\n",
      "       1.847, 1.846, 1.845, 1.845, 1.845, 1.843, 1.841, 1.84 , 1.838,\n",
      "       1.836, 1.837, 1.838, 1.836, 1.834, 1.831, 1.829, 1.827, 1.824,\n",
      "       1.822, 1.82 , 1.82 , 1.824, 1.825, 1.822, 1.819, 1.818, 1.819,\n",
      "       1.818, 1.813, 1.81 , 1.808, 1.806, 1.804, 1.808, 1.808, 1.808,\n",
      "       1.804, 1.804, 1.801, 1.799, 1.797, 1.796, 1.794, 1.789, 1.789,\n",
      "       1.784, 1.78 , 1.775, 1.772, 1.77 , 1.768, 1.765, 1.763, 1.761,\n",
      "       1.759, 1.757, 1.755, 1.752, 1.75 , 1.749, 1.743, 1.74 , 1.737,\n",
      "       1.735, 1.732, 1.729, 1.726, 1.725, 1.724, 1.722, 1.722, 1.72 ,\n",
      "       1.715, 1.712, 1.71 , 1.708, 1.705, 1.705, 1.705, 1.702, 1.7  ,\n",
      "       1.689, 1.684, 1.682, 1.685, 1.685, 1.682, 1.665, 1.664, 1.661,\n",
      "       1.659, 1.66 ])] [array([[0.00000000e+00, 9.25147322e-01, 0.00000000e+00, ...,\n",
      "        8.32391590e-01, 1.00000000e+00, 1.68692921e-03],\n",
      "       [1.00000000e+00, 9.25147322e-01, 0.00000000e+00, ...,\n",
      "        8.32391590e-01, 1.00000000e+00, 1.68692921e-03],\n",
      "       [2.00000000e+00, 9.25147322e-01, 0.00000000e+00, ...,\n",
      "        8.32391590e-01, 1.00000000e+00, 1.68692921e-03],\n",
      "       ...,\n",
      "       [2.91000000e+02, 1.43563972e-01, 7.71991078e-01, ...,\n",
      "        5.33112647e-01, 3.50708293e-02, 8.83834565e-01],\n",
      "       [2.92000000e+02, 9.17911921e-02, 7.60753962e-01, ...,\n",
      "        5.27188994e-01, 2.64062715e-02, 8.91396661e-01],\n",
      "       [2.93000000e+02, 6.49620888e-02, 7.00490447e-01, ...,\n",
      "        5.18324240e-01, 1.74666483e-02, 8.99889477e-01]]), array([1.953, 1.952, 1.951, 1.951, 1.951, 1.95 , 1.95 , 1.955, 1.956,\n",
      "       1.955, 1.955, 1.954, 1.954, 1.954, 1.954, 1.954, 1.953, 1.952,\n",
      "       1.954, 1.953, 1.952, 1.951, 1.951, 1.951, 1.95 , 1.95 , 1.949,\n",
      "       1.95 , 1.95 , 1.95 , 1.95 , 1.951, 1.951, 1.949, 1.95 , 1.949,\n",
      "       1.947, 1.946, 1.946, 1.946, 1.946, 1.946, 1.948, 1.95 , 1.95 ,\n",
      "       1.949, 1.949, 1.948, 1.948, 1.947, 1.946, 1.946, 1.946, 1.945,\n",
      "       1.946, 1.946, 1.947, 1.947, 1.946, 1.945, 1.945, 1.945, 1.944,\n",
      "       1.943, 1.944, 1.947, 1.947, 1.946, 1.946, 1.943, 1.943, 1.941,\n",
      "       1.941, 1.94 , 1.941, 1.946, 1.949, 1.947, 1.946, 1.945, 1.944,\n",
      "       1.943, 1.942, 1.942, 1.941, 1.94 , 1.941, 1.942, 1.941, 1.941,\n",
      "       1.939, 1.938, 1.937, 1.937, 1.936, 1.936, 1.935, 1.934, 1.934,\n",
      "       1.933, 1.933, 1.932, 1.932, 1.931, 1.931, 1.93 , 1.93 , 1.929,\n",
      "       1.928, 1.928, 1.927, 1.927, 1.925, 1.925, 1.924, 1.923, 1.922,\n",
      "       1.921, 1.919, 1.919, 1.918, 1.918, 1.916, 1.915, 1.914, 1.914,\n",
      "       1.913, 1.913, 1.912, 1.911, 1.91 , 1.909, 1.908, 1.905, 1.904,\n",
      "       1.903, 1.901, 1.9  , 1.898, 1.897, 1.895, 1.894, 1.892, 1.89 ,\n",
      "       1.89 , 1.888, 1.886, 1.884, 1.882, 1.882, 1.88 , 1.879, 1.878,\n",
      "       1.877, 1.875, 1.874, 1.873, 1.872, 1.872, 1.871, 1.869, 1.869,\n",
      "       1.868, 1.865, 1.859, 1.858, 1.857, 1.856, 1.855, 1.854, 1.852,\n",
      "       1.852, 1.851, 1.849, 1.848, 1.847, 1.848, 1.847, 1.845, 1.844,\n",
      "       1.842, 1.841, 1.841, 1.842, 1.84 , 1.835, 1.83 , 1.828, 1.827,\n",
      "       1.828, 1.832, 1.832, 1.829, 1.827, 1.826, 1.827, 1.821, 1.819,\n",
      "       1.817, 1.815, 1.814, 1.819, 1.82 , 1.819, 1.817, 1.816, 1.814,\n",
      "       1.813, 1.812, 1.811, 1.81 , 1.809, 1.808, 1.809, 1.809, 1.805,\n",
      "       1.805, 1.803, 1.799, 1.798, 1.797, 1.796, 1.793, 1.791, 1.791,\n",
      "       1.79 , 1.789, 1.787, 1.786, 1.787, 1.787, 1.787, 1.782, 1.776,\n",
      "       1.776, 1.776, 1.776, 1.774, 1.774, 1.769, 1.767, 1.765, 1.765,\n",
      "       1.766, 1.763, 1.756, 1.755, 1.753, 1.752, 1.75 , 1.749, 1.753,\n",
      "       1.754, 1.752, 1.736, 1.734, 1.733, 1.733, 1.734, 1.735, 1.732,\n",
      "       1.731, 1.728, 1.727, 1.722, 1.719, 1.715, 1.712, 1.71 , 1.707,\n",
      "       1.707, 1.707, 1.705, 1.703, 1.698, 1.695, 1.691, 1.687, 1.684,\n",
      "       1.68 , 1.676, 1.673, 1.671, 1.673, 1.667, 1.663, 1.659, 1.658,\n",
      "       1.651, 1.647, 1.644, 1.64 , 1.637, 1.636])]\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([32, 11])\n",
      "torch.Size([20, 11])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:40:12.488619Z",
     "start_time": "2025-03-11T02:40:12.461249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim=11, output_dim=12, num_heads=4, head_dim=24, dropout=0):\n",
    "        \"\"\"用多头注意力进行解码\"\"\"\n",
    "        \"\"\"\n",
    "        多头注意力模块。\n",
    "        :param input_dim: 输入特征维度\n",
    "        :param output_dim: 输出特征维度\n",
    "        :param num_heads: 注意力头的数量\n",
    "        :param head_dim: 每个注意力头的维度\n",
    "        :param dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.input_dim = input_dim-1\n",
    "        self.output_dim = output_dim-1\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.dropout = dropout\n",
    "        # 线性变换层，将输入映射到 Q, K, V\n",
    "        self.query = nn.Linear(input_dim-1, num_heads * head_dim)\n",
    "        self.key = nn.Linear(input_dim-1, num_heads * head_dim)\n",
    "        self.value = nn.Linear(input_dim-1, num_heads * head_dim)\n",
    "        # 输出线性层\n",
    "        self.fc_out = nn.Linear(num_heads * head_dim, output_dim-1)\n",
    "        # Dropout 层\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播。\n",
    "        :param x: 输入张量，形状为 (batch_size, input_dim)\n",
    "        :return: 输出张量，形状为 (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x_t=x[:,0].unsqueeze(1)\n",
    "        x=x[:,1:]\n",
    "        # 线性变换，得到 Q, K, V\n",
    "        Q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        K = self.key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)    # (batch_size, num_heads, seq_len, head_dim)\n",
    "        V = self.value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        attention_weights = self.dropout_layer(attention_weights)\n",
    "        # 计算加权和\n",
    "        attention_output = torch.matmul(attention_weights, V)  # (batch_size, num_heads, seq_len, head_dim)\n",
    "        # 拼接多头输出\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)  # (batch_size, seq_len, num_heads * head_dim)\n",
    "        # 通过线性层映射到输出维度\n",
    "        output = self.fc_out(attention_output)  # (batch_size, seq_len, output_dim)\n",
    "        output=torch.cat((x_t,output.squeeze(1)),dim=-1)\n",
    "        return output  # (batch_size, output_dim)\n",
    "\"\"\"--------------------------------------------------------多物理场混合专家模型-------------------------------------------------------\"\"\"\n",
    "class MixtureOfExperts(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, expert_hidden_dim):\n",
    "        super(MixtureOfExperts, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        # 专家网络\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, 2*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2*expert_hidden_dim, 4*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4*expert_hidden_dim, 8*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8*expert_hidden_dim, 16*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16*expert_hidden_dim, 32*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32*expert_hidden_dim,64*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64*expert_hidden_dim,32*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(32*expert_hidden_dim,16*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(16*expert_hidden_dim, 8*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(8*expert_hidden_dim, 4*expert_hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(4*expert_hidden_dim,expert_hidden_dim ),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # 门控网络\n",
    "        self.gating_network = nn.Linear(input_dim, num_experts)\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(expert_hidden_dim, 1)\n",
    "    def initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.gating_network.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "    def forward(self, x):\n",
    "        # 计算所有专家的输出\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n",
    "        #shape(batch_size,num_expert,expert_hidden_dim)\n",
    "        # 计算门控网络的输出并应用softmax得到权重\n",
    "        gate_works=torch.exp(self.gating_network(x)/10)\n",
    "        gating_outputs = F.softmax(gate_works, dim=1)\n",
    "        # 将门控网络的输出（权重）与专家网络的输出相乘并求和\n",
    "        combined_output = torch.sum(expert_outputs * gating_outputs.unsqueeze(-1), dim=1)\n",
    "        # 通过输出层得到最终输出\n",
    "        final_output = self.output_layer(combined_output)\n",
    "        return final_output,expert_outputs      #返回总输出和每个专家输出\n",
    "    \n",
    "class PINN_MOE(nn.Module):\n",
    "    def __init__(self,input_dim=11, output_dim=12, num_heads=4, head_dim=24, dropout=0,expert_input_dim=12, num_experts=3,expert_hidden_dim=2):\n",
    "        super(PINN_MOE, self).__init__()\n",
    "        self.Decoupling=MultiHeadAttention(input_dim, output_dim, num_heads, head_dim, dropout)\n",
    "        self.multi_physics=MixtureOfExperts(expert_input_dim, num_experts,expert_hidden_dim)\n",
    "        self.physics=nn.Sequential(\n",
    "            nn.Linear(6, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "        # self.electricity=nn.Sequential(nn.Linear(expert_hidden_dim, 2*expert_hidden_dim),nn.ReLU(),nn.Linear(2*expert_hidden_dim,expert_hidden_dim),nn.ReLU(),nn.Linear(expert_hidden_dim,2))\n",
    "        # self.heat=nn.Sequential(nn.Linear(expert_hidden_dim, 2*expert_hidden_dim),nn.ReLU(),nn.Linear(2*expert_hidden_dim,expert_hidden_dim),nn.ReLU(),nn.Linear(expert_hidden_dim,2))\n",
    "        # self.mechine=nn.Sequential(nn.Linear(expert_hidden_dim, 2*expert_hidden_dim),nn.ReLU(),nn.Linear(2*expert_hidden_dim,expert_hidden_dim),nn.ReLU(),nn.Linear(expert_hidden_dim,2))\n",
    "        self.parameter_heat=nn.Parameter(torch.tensor(1, dtype=torch.float32))\n",
    "        self.parameter_electricity1=nn.Parameter(torch.tensor(1, dtype=torch.float32))\n",
    "        self.parameter_electricity2=nn.Parameter(torch.tensor(1, dtype=torch.float32))\n",
    "    def  initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.Decoupling.parameters)\n",
    "        nn.init.xavier_uniform_(self.multi_physics.parameters)\n",
    "        # nn.init.xavier_uniform_(self.heat.parameters)\n",
    "        # nn.init.xavier_uniform_(self.mechine.parameters)\n",
    "        # nn.init.xavier_uniform_(self.electricity.parameters)\n",
    "        nn.init.xavier_uniform_(self.physics.parameters)\n",
    "        nn.init.xavier_uniform_(self.parameter_heat)\n",
    "        nn.init.xavier_uniform_(self.parameter_electricity1)\n",
    "        nn.init.xavier_uniform_(self.parameter_electricity2)\n",
    "    def forward(self, tx):\n",
    "        tx.requires_grad_(True)\n",
    "        # 解耦输入\n",
    "        t_x = self.Decoupling(tx)\n",
    "        t=t_x[:,0:1]\n",
    "        x=t_x[:,1:]\n",
    "        # 预测物理量\n",
    "        s_pred,experts = self.multi_physics(torch.cat((t,x),dim=1))\n",
    "        # 计算 s_pred 对 t 和 x 的偏导数\n",
    "        \"\"\"综合损失\"\"\"\n",
    "        s_t = grad(s_pred.sum(),t,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        s_x = grad(s_pred.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        \"\"\"热效应损失\"\"\"\n",
    "        T_Q=experts[:,0:1,:].squeeze(1)\n",
    "        # print(T_Q.shape)\n",
    "        T=T_Q[:,0:1]\n",
    "        Q=T_Q[:,1:2]\n",
    "        T_t=grad(T.sum(),t,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        T_x=grad(T.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        T_laplace=grad(T_t.sum(),t,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        loss_heat=torch.mean((T_t - (self.parameter_heat) * T_laplace - Q) ** 2, dim=1).unsqueeze(1)\n",
    "        \"\"\"电化学效应损失\"\"\"\n",
    "        phi_c=experts[:,1:2,:].squeeze(1)\n",
    "        phi=phi_c[:,0:1]\n",
    "        c=phi_c[:,1:2]\n",
    "        phi_t=grad(phi.sum(),t,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        phi_x=grad(phi.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        c_x=grad(c.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        c_t=grad(c.sum(),t,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        phi_laplace=grad(phi_x.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        c_laplace=grad(c_x.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        loss_electricity=torch.mean((c_t -self.parameter_electricity1 * c_laplace -self.parameter_electricity2) ** 2,dim=1).unsqueeze(1)+torch.mean(phi_laplace** 2,dim=1).unsqueeze(1)\n",
    "        \"\"\"机械应力损失\"\"\"\n",
    "        sigma_f=experts[:,2:3,:].squeeze(1)\n",
    "        sigma=sigma_f[:,0:1]\n",
    "        f=sigma_f[:,1:2]\n",
    "        sigma_x=grad(sigma.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        sigma_laplace=grad(sigma_x.sum(),x,create_graph=True,only_inputs=True,allow_unused=True)[0]\n",
    "        loss_mechine=torch.mean((sigma_laplace + f) ** 2,dim=1).unsqueeze(1)\n",
    "        # 打印 s_t 和 s_x，确保它们不为 None\n",
    "        # 计算物理约束 F\n",
    "        F_input = torch.cat([phi,c,T,Q,sigma,f], dim=1)\n",
    "        soh = self.physics(F_input)\n",
    "        # 计算残差 f\n",
    "        loss_all = 1*loss_electricity+1*loss_heat+1*loss_mechine\n",
    "        return soh, loss_all,[phi,c,T,Q,sigma,f]\n",
    "        "
   ],
   "id": "91f1315781131b82",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:40:27.524331Z",
     "start_time": "2025-03-11T02:40:27.513576Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(pretrained_model,lr=0.005,epochs=300, weight_decay=1e-4, seed=0, metric='rmse', device='cpu'):\n",
    "    \"\"\"function for train\"\"\"\n",
    "    setup_seed(seed)\n",
    "    print(\"training seed \"+str(seed)+':\\n')\n",
    "    train_loader, val_loader, test_35_data, test_36_data, test_37_data=get_data()\n",
    "    test_data=[test_35_data, test_36_data, test_37_data]\n",
    "    model = PINN_MOE()\n",
    "    model.load_state_dict(torch.load(pretrained_model))\n",
    "    model=model.to(device)\n",
    "    for param in model.multi_physics.parameters():\n",
    "        param.requires_grad = False\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.MSELoss()\n",
    "    len_dataloader = len(train_loader)\n",
    "    test_results=[]\n",
    "    lists=[]\n",
    "    train_loss=10\n",
    "    \"\"\"早停止获取最佳模型\"\"\"\n",
    "    val_mse=10\n",
    "    for epoch in range(epochs):\n",
    "        loss_epoch=0\n",
    "        for X,y in train_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_pred,f,_= model(X)\n",
    "            f_target = torch.zeros_like(f)\n",
    "            y_pred = y_pred.squeeze(1)\n",
    "            loss = criterion(y_pred,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_epoch += loss\n",
    "        if (epoch+1)%5==0 and epoch!=0:\n",
    "            print('Epoch:',epoch+1,'Train_RMSELoss:',loss_epoch/len_dataloader,'\\n')\n",
    "            train_loss=loss_epoch/len_dataloader\n",
    "        if (epoch+1)%5==0 and epoch!=0:\n",
    "            val_loss=0\n",
    "            for val_x,val_y in val_loader:\n",
    "                val_x,val_y=val_x.to(device),val_y.to(device)\n",
    "                pre,_,_=model(val_x)\n",
    "                val_loss+=criterion(pre.squeeze(1),val_y).detach()    \n",
    "            print('Epoch:',epoch+1,'valid_RMSELoss:',val_loss/len(val_loader),'\\n')\n",
    "            val_mse=val_loss/len(val_loader)\n",
    "                \n",
    "        if (val_mse<10e-6 and epoch>30)or (epoch+1)==epochs:\n",
    "            model=model.cpu()\n",
    "            for name in test_data:\n",
    "                X=torch.from_numpy(name[0]).float()\n",
    "                y=name[1]\n",
    "                y_pred,_,list=model(X)\n",
    "                for i in range(len(list)):\n",
    "                    list[i]=list[i].detach().cpu().numpy()\n",
    "                lists.append(list)\n",
    "                y_pred= y_pred.squeeze(0).detach().numpy()\n",
    "                test_results.append([y,y_pred])\n",
    "            break\n",
    "    return test_results,lists"
   ],
   "id": "7c87c683cc1615f0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HUST",
   "id": "2f95a0d085375fcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:43:52.557602Z",
     "start_time": "2025-03-11T02:42:35.786361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model='../pretrained/hust_model.pth'\n",
    "test_results,_=train(pretrained_model,seed=0)\n",
    "for i in range(3):\n",
    "    real,pred=test_results[i][0],test_results[i][1]\n",
    "    rmse=evaluation(real,pred)\n",
    "    print(rmse)"
   ],
   "id": "38ca9bb740d7763b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_13492\\3217803037.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train_RMSELoss: tensor(0.0019, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 5 valid_RMSELoss: tensor(0.0016) \n",
      "\n",
      "Epoch: 10 Train_RMSELoss: tensor(0.0008, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 10 valid_RMSELoss: tensor(0.0008) \n",
      "\n",
      "Epoch: 15 Train_RMSELoss: tensor(0.0004, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 15 valid_RMSELoss: tensor(0.0005) \n",
      "\n",
      "Epoch: 20 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 20 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 25 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 25 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 30 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 30 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 35 Train_RMSELoss: tensor(9.1341e-05, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 35 valid_RMSELoss: tensor(8.8247e-05) \n",
      "\n",
      "Epoch: 40 Train_RMSELoss: tensor(7.4581e-05, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 40 valid_RMSELoss: tensor(7.9972e-05) \n",
      "\n",
      "Epoch: 45 Train_RMSELoss: tensor(9.6100e-05, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 45 valid_RMSELoss: tensor(8.8750e-05) \n",
      "\n",
      "Epoch: 50 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 50 valid_RMSELoss: tensor(9.9601e-05) \n",
      "\n",
      "Epoch: 55 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 55 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 60 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 60 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 65 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 65 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 70 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 70 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 75 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 75 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 80 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 80 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 85 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 85 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 90 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 90 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 95 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 95 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 100 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 100 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 105 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 105 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 110 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 110 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 115 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 115 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 120 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 120 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 125 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 125 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 130 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 130 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 135 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 135 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 140 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 140 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 145 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 145 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 150 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 150 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 155 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 155 valid_RMSELoss: tensor(0.0004) \n",
      "\n",
      "Epoch: 160 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 160 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 165 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 165 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 170 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 170 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 175 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 175 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 180 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 180 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 185 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 185 valid_RMSELoss: tensor(9.0766e-05) \n",
      "\n",
      "Epoch: 190 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 190 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 195 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 195 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 200 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 200 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 205 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 205 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 210 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 210 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 215 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 215 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 220 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 220 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 225 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 225 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 230 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 230 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 235 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 235 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 240 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 240 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 245 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 245 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 250 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 250 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 255 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 255 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 260 Train_RMSELoss: tensor(9.5356e-05, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 260 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 265 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 265 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 270 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 270 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 275 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 275 valid_RMSELoss: tensor(8.9653e-05) \n",
      "\n",
      "Epoch: 280 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 280 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 285 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 285 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 290 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 290 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 295 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 295 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 300 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 300 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "0.011399797749062295\n",
      "0.010934340109948046\n",
      "0.014456202693309767\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CACLE",
   "id": "41b931595730a10f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T02:46:52.706376Z",
     "start_time": "2025-03-11T02:45:28.291416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model='../pretrained/cacle_model.pth'\n",
    "test_results,_=train(pretrained_model,seed=0)\n",
    "for i in range(3):\n",
    "    real,pred=test_results[i][0],test_results[i][1]\n",
    "    rmse=evaluation(real,pred)\n",
    "    print(rmse)"
   ],
   "id": "e329d73e0090d2c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training seed 0:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Local\\Temp\\ipykernel_13492\\3217803037.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(pretrained_model))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Train_RMSELoss: tensor(0.0081, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 5 valid_RMSELoss: tensor(0.0089) \n",
      "\n",
      "Epoch: 10 Train_RMSELoss: tensor(0.0028, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 10 valid_RMSELoss: tensor(0.0023) \n",
      "\n",
      "Epoch: 15 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 15 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 20 Train_RMSELoss: tensor(0.0007, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 20 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 25 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 25 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 30 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 30 valid_RMSELoss: tensor(0.0008) \n",
      "\n",
      "Epoch: 35 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 35 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 40 Train_RMSELoss: tensor(0.0008, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 40 valid_RMSELoss: tensor(0.0008) \n",
      "\n",
      "Epoch: 45 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 45 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 50 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 50 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 55 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 55 valid_RMSELoss: tensor(0.0007) \n",
      "\n",
      "Epoch: 60 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 60 valid_RMSELoss: tensor(0.0007) \n",
      "\n",
      "Epoch: 65 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 65 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 70 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 70 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 75 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 75 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 80 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 80 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 85 Train_RMSELoss: tensor(0.0007, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 85 valid_RMSELoss: tensor(0.0010) \n",
      "\n",
      "Epoch: 90 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 90 valid_RMSELoss: tensor(0.0007) \n",
      "\n",
      "Epoch: 95 Train_RMSELoss: tensor(0.0006, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 95 valid_RMSELoss: tensor(0.0006) \n",
      "\n",
      "Epoch: 100 Train_RMSELoss: tensor(0.0004, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 100 valid_RMSELoss: tensor(0.0004) \n",
      "\n",
      "Epoch: 105 Train_RMSELoss: tensor(0.0004, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 105 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 110 Train_RMSELoss: tensor(0.0004, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 110 valid_RMSELoss: tensor(0.0004) \n",
      "\n",
      "Epoch: 115 Train_RMSELoss: tensor(0.0003, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 115 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 120 Train_RMSELoss: tensor(0.0003, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 120 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 125 Train_RMSELoss: tensor(0.0003, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 125 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 130 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 130 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 135 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 135 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 140 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 140 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 145 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 145 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 150 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 150 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 155 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 155 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 160 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 160 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 165 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 165 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 170 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 170 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 175 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 175 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 180 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 180 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 185 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 185 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 190 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 190 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 195 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 195 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 200 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 200 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 205 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 205 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 210 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 210 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 215 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 215 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 220 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 220 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 225 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 225 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 230 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 230 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 235 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 235 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 240 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 240 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 245 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 245 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 250 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 250 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 255 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 255 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 260 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 260 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "Epoch: 265 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 265 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 270 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 270 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 275 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 275 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 280 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 280 valid_RMSELoss: tensor(0.0002) \n",
      "\n",
      "Epoch: 285 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 285 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 290 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 290 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 295 Train_RMSELoss: tensor(0.0002, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 295 valid_RMSELoss: tensor(0.0001) \n",
      "\n",
      "Epoch: 300 Train_RMSELoss: tensor(0.0001, grad_fn=<DivBackward0>) \n",
      "\n",
      "Epoch: 300 valid_RMSELoss: tensor(0.0003) \n",
      "\n",
      "0.011465072721149059\n",
      "0.012491112642297693\n",
      "0.021385332740338617\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a8708a070ed9b88"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
